{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Please place everything in `drive/MyDrive/nlpkiso`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-A5VNfbZ-Cl",
        "outputId": "8751cc8a-db0b-4e2f-8823-5bfd2bb9958b"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/nlpkiso')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5Hr-uf5UUtX"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('popular')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download news+aggregator data & preprocess it\n",
        "\n",
        "! bash data/main.sh\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`GoogleNews-vectors-negative300.bin` should be downloaded separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4FWWAYO2wtRR"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "vectors = KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "hWoViS6tZagy"
      },
      "outputs": [],
      "source": [
        "# Prepare vocabulary\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "counter = Counter()\n",
        "with open(\"data/train.txt\") as f:\n",
        "    for line in f:\n",
        "        _, title = line.rstrip(\"\\n\").split(\"\\t\")\n",
        "        tokens = nltk.tokenize.word_tokenize(title)\n",
        "        counter.update(tokens)\n",
        "\n",
        "vocab_in_train_list = sorted(counter.keys(), key=lambda x: counter[x], reverse=True)\n",
        "vocab_list = [\"[UNK]\"] + vocab_in_train_list\n",
        "vocab_dict = {vocab: id for id, vocab in enumerate(vocab_list)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dEQPJ024pBnu"
      },
      "outputs": [],
      "source": [
        "# Define functions used to prepare datasets\n",
        "\n",
        "import torch\n",
        "\n",
        "categories = [\"b\", \"t\", \"e\", \"m\"]\n",
        "\n",
        "\n",
        "def sent_to_ids(sent):\n",
        "    return torch.tensor(\n",
        "        [vocab_dict[x if x in vocab_dict else \"[UNK]\"] for x in sent], dtype=torch.long\n",
        "    )\n",
        "\n",
        "\n",
        "def dataset_to_ids(dataset):\n",
        "    return [sent_to_ids(x) for x in dataset]\n",
        "\n",
        "\n",
        "def read_feature_dataset(filename):\n",
        "    with open(filename) as f:\n",
        "        dataset = f.read().splitlines()\n",
        "    dataset = [line.split(\"\\t\") for line in dataset]\n",
        "    dataset_t = [categories.index(line[0]) for line in dataset]\n",
        "    dataset_x = [nltk.tokenize.word_tokenize(line[1]) for line in dataset]\n",
        "    return dataset_x, torch.tensor(dataset_t, dtype=torch.long)\n",
        "\n",
        "\n",
        "def init_embed(embed):\n",
        "    for i, token in enumerate(vocab_list):\n",
        "        if token in vectors:\n",
        "            embed.weight.data[i] = torch.from_numpy(vectors[token])\n",
        "    return embed\n",
        "\n",
        "\n",
        "class NewsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_ids, data_y, phase=\"train\"):\n",
        "        self.X = data_ids\n",
        "        self.y = data_y\n",
        "        self.phase = phase\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inputs = torch.tensor(self.X[idx])\n",
        "        return inputs, self.y[idx]\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    sequences = [x[0] for x in batch]\n",
        "    labels = torch.LongTensor([x[1] for x in batch])\n",
        "    x = torch.nn.utils.rnn.pad_sequence(\n",
        "        sequences, batch_first=True, padding_value=PADDING_IDX\n",
        "    )\n",
        "    return x, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "vYFSBegOqhPJ"
      },
      "outputs": [],
      "source": [
        "# Create dataset\n",
        "\n",
        "train_X, train_y = read_feature_dataset(\"data/train.txt\")\n",
        "valid_X, valid_y = read_feature_dataset(\"data/valid.txt\")\n",
        "test_X, test_y = read_feature_dataset(\"data/test.txt\")\n",
        "\n",
        "train_ids = dataset_to_ids(train_X)\n",
        "valid_ids = dataset_to_ids(valid_X)\n",
        "test_ids = dataset_to_ids(test_X)\n",
        "\n",
        "train_dataset = NewsDataset(train_ids, train_y, phase=\"train\")\n",
        "valid_dataset = NewsDataset(valid_ids, valid_y, phase=\"valid\")\n",
        "test_dataset = NewsDataset(test_ids, test_y, phase=\"test\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "z3nsMKgl7nK8"
      },
      "outputs": [],
      "source": [
        "# Define parameters of RNN and LSTM\n",
        "\n",
        "import numpy as np\n",
        "import src.models as models\n",
        "from src.set_seed import seed_everything, seed_worker\n",
        "\n",
        "seed = 42\n",
        "seed_everything(seed)\n",
        "\n",
        "g = torch.Generator()\n",
        "g.manual_seed(seed)\n",
        "\n",
        "VOCAB_SIZE = len(set(vocab_dict.values())) + 2\n",
        "EMB_SIZE = 300\n",
        "PADDING_IDX = len(set(vocab_dict.values())) + 1\n",
        "OUTPUT_SIZE = 4\n",
        "HIDDEN_SIZE = 50\n",
        "NUM_LAYERS = 1\n",
        "\n",
        "weights = np.zeros((VOCAB_SIZE, EMB_SIZE))\n",
        "words_in_pretrained = 0\n",
        "for i, word in enumerate(vocab_dict.keys()):\n",
        "    try:\n",
        "        weights[i] = vectors[word]\n",
        "        words_in_pretrained += 1\n",
        "    except KeyError:\n",
        "        weights[i] = np.random.normal(scale=0.1, size=(EMB_SIZE,))\n",
        "weights = torch.from_numpy(weights.astype((np.float32)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cX-4G8ojWAX",
        "outputId": "30b24e6f-d3b0-470d-aadf-fc59612d980b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RNN(\n",
            "  (emb): Embedding(18833, 300, padding_idx=18832)\n",
            "  (rnn): RNN(300, 50, batch_first=True)\n",
            "  (fc): Linear(in_features=50, out_features=4, bias=True)\n",
            ")\n",
            "LSTM(\n",
            "  (emb): Embedding(18833, 300, padding_idx=18832)\n",
            "  (lstm): LSTM(300, 50, batch_first=True)\n",
            "  (fc): Linear(in_features=50, out_features=4, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Create models\n",
        "\n",
        "rnn_model = models.RNN(\n",
        "    VOCAB_SIZE, EMB_SIZE, PADDING_IDX, HIDDEN_SIZE, OUTPUT_SIZE, NUM_LAYERS, weights\n",
        ")\n",
        "lstm_model = models.LSTM(\n",
        "    VOCAB_SIZE, EMB_SIZE, PADDING_IDX, HIDDEN_SIZE, OUTPUT_SIZE, NUM_LAYERS, weights\n",
        ")\n",
        "print(rnn_model)\n",
        "print(lstm_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "tnr0p2_z6i3b"
      },
      "outputs": [],
      "source": [
        "# Create dataloader\n",
        "\n",
        "import torch.utils.data as data\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "train_dataloader = data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    worker_init_fn=seed_worker,\n",
        "    generator=g,\n",
        ")\n",
        "valid_dataloader = data.DataLoader(\n",
        "    valid_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        "    worker_init_fn=seed_worker,\n",
        "    generator=g,\n",
        ")\n",
        "test_dataloader = data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        "    worker_init_fn=seed_worker,\n",
        "    generator=g,\n",
        ")\n",
        "\n",
        "dataloaders_dict = {\n",
        "    \"train\": train_dataloader,\n",
        "    \"val\": valid_dataloader,\n",
        "    \"test\": test_dataloader,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ithNz4RDET8",
        "outputId": "b9c75c9f-a56d-47c6-fb6c-0852f1255bf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tesla T4\n",
            "device: cuda:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-6-66516abfb07c>:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  inputs = torch.tensor(self.X[idx])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 / 10 (train) Loss: 1.2120, Acc: 0.4418, (val) Loss: 1.1499, Acc: 0.4708\n",
            "Epoch 2 / 10 (train) Loss: 1.2349, Acc: 0.4493, (val) Loss: 1.2537, Acc: 0.3990\n",
            "Epoch 3 / 10 (train) Loss: 1.2468, Acc: 0.4520, (val) Loss: 1.2496, Acc: 0.4214\n",
            "Epoch 4 / 10 (train) Loss: 1.2451, Acc: 0.4227, (val) Loss: 1.3605, Acc: 0.3960\n",
            "Epoch 5 / 10 (train) Loss: 1.2812, Acc: 0.4149, (val) Loss: 1.2505, Acc: 0.4214\n",
            "Epoch 6 / 10 (train) Loss: 1.2578, Acc: 0.4179, (val) Loss: 1.3009, Acc: 0.3960\n",
            "Epoch 7 / 10 (train) Loss: 1.2553, Acc: 0.4277, (val) Loss: 1.7413, Acc: 0.3960\n",
            "Epoch 8 / 10 (train) Loss: 1.2692, Acc: 0.4265, (val) Loss: 1.1621, Acc: 0.3915\n",
            "Epoch 9 / 10 (train) Loss: 1.2521, Acc: 0.4354, (val) Loss: 1.2210, Acc: 0.3960\n",
            "Epoch 10 / 10 (train) Loss: 1.2552, Acc: 0.4190, (val) Loss: 1.2011, Acc: 0.4528\n"
          ]
        }
      ],
      "source": [
        "# Train RNN model\n",
        "\n",
        "import torch.nn as nn\n",
        "from src.trainer import train_model\n",
        "\n",
        "rnn_model.train()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(rnn_model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "rnn_reports = dict()\n",
        "(\n",
        "    rnn_reports[\"train_loss\"],\n",
        "    rnn_reports[\"train_acc\"],\n",
        "    rnn_reports[\"valid_loss\"],\n",
        "    rnn_reports[\"valid_acc\"],\n",
        ") = train_model(\n",
        "    rnn_model, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XYykwdE7Q07",
        "outputId": "c7eaf00b-5f83-4e44-b120-0c5d5ec714e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tesla T4\n",
            "device: cuda:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-6-66516abfb07c>:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  inputs = torch.tensor(self.X[idx])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 / 10 (train) Loss: 0.7401, Acc: 0.7054, (val) Loss: 0.5043, Acc: 0.8069\n",
            "Epoch 2 / 10 (train) Loss: 0.4468, Acc: 0.8390, (val) Loss: 0.7603, Acc: 0.6916\n",
            "Epoch 3 / 10 (train) Loss: 0.3494, Acc: 0.8800, (val) Loss: 0.3390, Acc: 0.8780\n",
            "Epoch 4 / 10 (train) Loss: 0.2969, Acc: 0.8997, (val) Loss: 0.3268, Acc: 0.8937\n",
            "Epoch 5 / 10 (train) Loss: 0.2667, Acc: 0.9105, (val) Loss: 0.3592, Acc: 0.8728\n",
            "Epoch 6 / 10 (train) Loss: 0.2395, Acc: 0.9179, (val) Loss: 0.3877, Acc: 0.8488\n",
            "Epoch 7 / 10 (train) Loss: 0.2239, Acc: 0.9255, (val) Loss: 0.3363, Acc: 0.8787\n",
            "Epoch 8 / 10 (train) Loss: 0.2077, Acc: 0.9325, (val) Loss: 0.2616, Acc: 0.9132\n",
            "Epoch 9 / 10 (train) Loss: 0.1910, Acc: 0.9351, (val) Loss: 0.2744, Acc: 0.9019\n",
            "Epoch 10 / 10 (train) Loss: 0.1818, Acc: 0.9393, (val) Loss: 0.2932, Acc: 0.8975\n"
          ]
        }
      ],
      "source": [
        "# Train LSTM model\n",
        "\n",
        "lstm_model.train()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(lstm_model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "num_epochs = 10\n",
        "lstm_reports = dict()\n",
        "(\n",
        "    lstm_reports[\"train_loss\"],\n",
        "    lstm_reports[\"train_acc\"],\n",
        "    lstm_reports[\"valid_loss\"],\n",
        "    lstm_reports[\"valid_acc\"],\n",
        ") = train_model(\n",
        "    lstm_model, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "t7tCQhuTEVJb",
        "outputId": "029735c3-00d6-4a2b-f715-00ce6cdbbd45"
      },
      "outputs": [],
      "source": [
        "# Visualize results\n",
        "\n",
        "from src.visualizer import visualize\n",
        "\n",
        "visualize(rnn_reports, lstm_reports)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
